{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / np.exp(-x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "    '''\n",
        "    Split training_datalist into training_dataset (~80%) and validation_dataset (~20%)\n",
        "    '''\n",
        "    training_list = pd.DataFrame(training_datalist[1:], columns = training_datalist[0]).astype('int')\n",
        "    training_dataset = training_list[:290].reset_index()\n",
        "    validation_dataset = training_list[290:].reset_index()\n",
        "    return training_dataset, validation_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(training_dataset, validation_dataset):\n",
        "    '''\n",
        "    Handle outlier by using interquartile range (IQR)\n",
        "    '''\n",
        "    Q1_x = np.percentile(training_dataset['dbp'], 25)\n",
        "    Q3_x = np.percentile(training_dataset['dbp'], 75)\n",
        "    IQR_x = Q3_x - Q1_x\n",
        "    lower_limit_x = Q1_x - (1.5 * IQR_x)\n",
        "    upper_limit_x = Q3_x + (1.5 * IQR_x)\n",
        "    training_dataset = training_dataset[(training_dataset['dbp'] < upper_limit_x)]\n",
        "    Q1_y = np.percentile(training_dataset['sbp'], 25)\n",
        "    Q3_y = np.percentile(training_dataset['sbp'], 75)\n",
        "    IQR_y = Q3_y - Q1_y\n",
        "    lower_limit_y = Q1_y - (1.5 * IQR_y)\n",
        "    upper_limit_y = Q3_y + (1.5 * IQR_y)\n",
        "    training_dataset = training_dataset[(training_dataset['sbp'] < upper_limit_y)].reset_index()\n",
        "    x_train, y_train = training_dataset['dbp'], training_dataset['sbp']\n",
        "    return x_train, y_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def MatrixInversion(x_train, y_train, max_degrees):\n",
        "    '''\n",
        "    Find weight (w) of each feature (phi) by using matrix inversion\n",
        "    x_train: dbp value of training data\n",
        "    y_train: sbp value of training data\n",
        "    max_degrees: maximum degree of polynomial\n",
        "    '''\n",
        "    phi = np.array([])\n",
        "    a = []\n",
        "    a.append(np.log(x_train[0] + 1))\n",
        "    a.append(sigmoid(x_train[0]))\n",
        "    for i in range(max_degrees, -1, -1):\n",
        "        a.append(np.power(x_train[0], i))\n",
        "    phi = np.hstack((phi, a))\n",
        "    for i in range(1, len(x_train)):\n",
        "        a.clear()\n",
        "        a.append(np.log(x_train[i] + 1))\n",
        "        a.append(sigmoid(x_train[i]))\n",
        "        for j in range(max_degrees, -1, -1):\n",
        "            a.append(np.power(x_train[i], j))\n",
        "        phi = np.vstack((phi, a))\n",
        "    w = np.linalg.inv((np.transpose(phi) @ phi)) @ np.transpose(phi) @ y_train\n",
        "    return w\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(w, max_degrees):\n",
        "    '''\n",
        "    Make prediction of testing_datalist\n",
        "    w: weight of each feature (phi)\n",
        "    max_degrees: maximum degree of polynomial\n",
        "    '''\n",
        "    testing_list = pd.DataFrame(testing_datalist[1:], columns = testing_datalist[0]).astype('int')\n",
        "    x_test = testing_list['dbp']\n",
        "    a = 0\n",
        "    output_datalist = []\n",
        "    a += w[0] * np.log(x_test[0] + 1)\n",
        "    a += w[1] * sigmoid(x_test[0])\n",
        "    for i in range(max_degrees, -1, -1):\n",
        "        a += w[max_degrees - i + 2] * np.power(x_test[0], i)\n",
        "    output_datalist = np.hstack((output_datalist, [a]))\n",
        "    for i in range(1, len(x_test)):\n",
        "        a = 0\n",
        "        a += w[0] * np.log(x_test[i] + 1)\n",
        "        a += w[1] * sigmoid(x_test[i])\n",
        "        for j in range(max_degrees, -1, -1):\n",
        "            a += w[max_degrees - j + 2] * np.power(x_test[i], j) \n",
        "        output_datalist = np.hstack((output_datalist, [a]))\n",
        "    return output_datalist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iCL92EPKOFIn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-3.70324648e+01 -9.82052221e-49  1.41813979e+00  1.76015008e+02]\n"
          ]
        }
      ],
      "source": [
        "max_degrees = 1\n",
        "\n",
        "training_dataset, validation_dataset = SplitData()\n",
        "x_train, y_train = PreprocessData(training_dataset, validation_dataset)\n",
        "w = MatrixInversion(x_train, y_train, max_degrees)\n",
        "output_datalist = []\n",
        "output = MakePrediction(w, max_degrees)\n",
        "for i in range(len(output)):\n",
        "    output_datalist.append([output[i]])\n",
        "    \n",
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "    '''\n",
        "    Split training_datalist into training_dataset (~80%) and validation_dataset (~20%)\n",
        "    '''\n",
        "    training_list = pd.DataFrame(training_datalist[1:], columns = training_datalist[0]).astype('int')\n",
        "    training_dataset = training_list[:290].reset_index()\n",
        "    validation_dataset = training_list[290:].reset_index()\n",
        "    return training_dataset, validation_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(training_dataset, validation_dataset):\n",
        "    '''\n",
        "    Handle outlier by using interquartile range (IQR)\n",
        "    '''\n",
        "    Q1_x = np.percentile(training_dataset['dbp'], 25)\n",
        "    Q3_x = np.percentile(training_dataset['dbp'], 75)\n",
        "    IQR_x = Q3_x - Q1_x\n",
        "    lower_limit_x = Q1_x - (1.5 * IQR_x)\n",
        "    upper_limit_x = Q3_x + (1.5 * IQR_x)\n",
        "    training_dataset = training_dataset[(training_dataset['dbp'] < upper_limit_x)]\n",
        "    Q1_y = np.percentile(training_dataset['sbp'], 25)\n",
        "    Q3_y = np.percentile(training_dataset['sbp'], 75)\n",
        "    IQR_y = Q3_y - Q1_y\n",
        "    lower_limit_y = Q1_y - (1.5 * IQR_y)\n",
        "    upper_limit_y = Q3_y + (1.5 * IQR_y)\n",
        "    training_dataset = training_dataset[(training_dataset['sbp'] < upper_limit_y)].reset_index()\n",
        "    x_train, y_train = training_dataset['dbp'], training_dataset['sbp']\n",
        "    return x_train, y_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "def GradientDescent(x_train, y_train, max_degrees):\n",
        "    '''\n",
        "    Find weight (w) of each feature (phi) by using gradient descent\n",
        "    x_train: dbp value of training data\n",
        "    y_train: sbp value of training data\n",
        "    max_degrees: maximum degree of polynomial\n",
        "    '''\n",
        "    phi = np.array([])\n",
        "    a = []\n",
        "    a.append(np.log(x_train[0] + 1))\n",
        "    for i in range(max_degrees, -1, -1):\n",
        "        a.append(np.power(x_train[0], i))\n",
        "    phi = np.hstack((phi, a))\n",
        "    for i in range(1, len(x_train)):\n",
        "        a.clear()\n",
        "        a.append(np.log(x_train[i] + 1))\n",
        "        for j in range(max_degrees, -1, -1):\n",
        "            a.append(np.power(x_train[i], j))\n",
        "        phi = np.vstack((phi, a))\n",
        "    w = np.transpose(np.zeros(max_degrees + 2))\n",
        "    lr = 6e-8\n",
        "    coefficient_output.append(w)\n",
        "    for i in range(100000):\n",
        "        y_predict = phi @ w\n",
        "        g = -2 * (np.transpose(phi) @ np.transpose(np.array(y_train) - y_predict))\n",
        "        w = w - lr * g\n",
        "        coefficient_output.append(w)\n",
        "    return w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(w, max_degrees):\n",
        "    '''\n",
        "    Make prediction of testing_datalist\n",
        "    w: weight of each feature (phi)\n",
        "    max_degrees: maximum degree of polynomial\n",
        "    '''\n",
        "    testing_list = pd.DataFrame(testing_datalist[1:], columns = testing_datalist[0]).astype('int')\n",
        "    x_test = testing_list['dbp']\n",
        "    a = 0\n",
        "    output_datalist = []\n",
        "    a += w[0] * np.log(x_test[0] + 1)\n",
        "    for i in range(max_degrees, -1, -1):\n",
        "        a += w[max_degrees - i + 1] * np.power(x_test[0], i)\n",
        "    output_datalist = np.hstack((output_datalist, [a]))\n",
        "    for i in range(1, len(x_test)):\n",
        "        a = 0\n",
        "        a += w[0] * np.log(x_test[i] + 1)\n",
        "        for j in range(max_degrees, -1, -1):\n",
        "            a += w[max_degrees - j + 1] * np.power(x_test[i], j) \n",
        "        output_datalist = np.hstack((output_datalist, [a]))\n",
        "    return output_datalist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "90EisOc7YG-N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9.36266875 1.02979002 2.91600117]\n"
          ]
        }
      ],
      "source": [
        "max_degrees = 1\n",
        "\n",
        "training_dataset, validation_dataset = SplitData()\n",
        "x_train, y_train = PreprocessData(training_dataset, validation_dataset)\n",
        "w = GradientDescent(x_train, y_train, max_degrees)\n",
        "output_datalist = []\n",
        "output = MakePrediction(w, max_degrees)\n",
        "for i in range(len(output)):\n",
        "    output_datalist.append([output[i]])\n",
        "\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in coefficient_output:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### *Load the Input File*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### *Implement the Regression Model*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def SplitData(id, training_list, shift_time):\n",
        "    '''\n",
        "    Split training_datalist into training_dataset (~80%) and validation_dataset (~20%)\n",
        "    id: subject_id in training_datalist\n",
        "    training_list: training_datalist\n",
        "    shift_time: number of shift in sbp column\n",
        "    '''\n",
        "    for i in range(1, shift_time + 1):\n",
        "        training_list[f'sbp[t-{i}]'] = training_list['sbp'].shift(i)\n",
        "    training_list = training_list.fillna(method='bfill')\n",
        "    subject_training_dataset = training_list.loc[training_list['subject_id'] == f'{id}'].reset_index()\n",
        "    train_length = int(len(subject_training_dataset) * 0.8)\n",
        "    training_dataset = subject_training_dataset[:train_length]\n",
        "    validation_dataset = subject_training_dataset[train_length:]\n",
        "    return training_dataset, validation_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PreprocessData(training_dataset, validation_dataset, shift_time):\n",
        "    '''\n",
        "    Handle missing data by deleting and outlier by using interquartile range (IQR)\n",
        "    '''\n",
        "    training_dataset = training_dataset[(training_dataset['heartrate'] != '')]\n",
        "    training_dataset['heartrate'] = training_dataset['heartrate'].astype('int')\n",
        "    training_dataset['sbp'] = training_dataset['sbp'].astype('int')\n",
        "    Q1_x = np.percentile(training_dataset['heartrate'], 25)\n",
        "    Q3_x = np.percentile(training_dataset['heartrate'], 75)\n",
        "    IQR_x = Q3_x - Q1_x\n",
        "    lower_limit_x = Q1_x - (1.5 * IQR_x)\n",
        "    upper_limit_x = Q3_x + (1.5 * IQR_x)\n",
        "    training_dataset = training_dataset[(training_dataset['heartrate'].astype('int') < upper_limit_x)]\n",
        "    Q1_y = np.percentile(training_dataset['sbp'], 25)\n",
        "    Q3_y = np.percentile(training_dataset['sbp'], 75)\n",
        "    IQR_y = Q3_y - Q1_y\n",
        "    lower_limit_y = Q1_y - (1.5 * IQR_y)\n",
        "    upper_limit_y = Q3_y + (1.5 * IQR_y)\n",
        "    training_dataset = training_dataset[(training_dataset['sbp'] < upper_limit_y)].reset_index()\n",
        "    x_train = training_dataset['heartrate'].astype('int')\n",
        "    for i in range(1, shift_time + 1):\n",
        "        x_train = pd.concat([x_train, training_dataset[f'sbp[t-{i}]']], axis=1)\n",
        "    x_train = x_train.astype('int')\n",
        "    y_train = training_dataset['sbp']\n",
        "    return x_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3: Implement Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MatrixInversion(x_train, y_train, shift_time, max_degrees):\n",
        "    '''\n",
        "    Find weight (w) of each feature (phi) by using matrix inversion\n",
        "    x_train: heartrate value of training data\n",
        "    y_train: sbp value of training data\n",
        "    shift_time: number of shift in sbp column\n",
        "    max_degrees: maximum degree of polynomial\n",
        "    '''\n",
        "    phi = np.array([])\n",
        "    a = []\n",
        "    for i in range(1, shift_time + 1):\n",
        "        a.append(x_train[f'sbp[t-{i}]'][0])\n",
        "    for i in range(max_degrees, -1, -1):\n",
        "        a.append(np.power(x_train['heartrate'][0], i))\n",
        "    phi = np.hstack((phi, a))\n",
        "    for i in range(1, len(x_train)):\n",
        "        a.clear()\n",
        "        for j in range(1, shift_time + 1):\n",
        "            a.append(x_train[f'sbp[t-{j}]'][i])\n",
        "        for k in range(max_degrees, -1, -1):\n",
        "            a.append(np.power(x_train['heartrate'][i], k))\n",
        "        phi = np.vstack((phi, a))\n",
        "        a.clear()\n",
        "    w = np.linalg.inv((np.transpose(phi) @ phi)) @ np.transpose(phi) @ y_train\n",
        "    return w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4: Train Model and Make Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MakePrediction(dataset, shift_time, max_degrees, w):\n",
        "    '''\n",
        "    Make prediction of dataset\n",
        "    dataset: all data\n",
        "    shift_time: number of shift in sbp column\n",
        "    max_degrees: maximum degree of polynomial\n",
        "    w: weight of each feature (phi)\n",
        "    '''\n",
        "    dataset['heartrate'] = dataset['heartrate'].astype('int')\n",
        "    for i in range(1, shift_time + 1):\n",
        "        dataset[f'sbp[t-{i}]'] = dataset[f'sbp[t-{i}]'].astype('int')\n",
        "    dataset['sbp'] = dataset['sbp'].astype('int')\n",
        "    y_predict = np.array([])\n",
        "\n",
        "    a = 0\n",
        "    for i in range(1, shift_time + 1):\n",
        "        a += w[i - 1] * dataset[f'sbp[t-{i}]'][0]\n",
        "    for i in range(max_degrees, -1, -1):\n",
        "        a += w[max_degrees - i + shift_time] * np.power(dataset['heartrate'][0], i)\n",
        "    y_predict = np.hstack((y_predict, [a]))\n",
        "    for i in range(1, shift_time + 1):\n",
        "        dataset[f'sbp[t-{i}]'][i] = a\n",
        "\n",
        "    for i in range(1, len(dataset['heartrate'])):\n",
        "        a = 0\n",
        "        for j in range(1, shift_time + 1):\n",
        "            a += w[j - 1] * dataset[f'sbp[t-{j}]'][i]\n",
        "        for k in range(max_degrees, -1, -1):\n",
        "            a += w[max_degrees - k + shift_time] * np.power(dataset['heartrate'][i], k) \n",
        "        y_predict = np.hstack((y_predict, [a]))\n",
        "        for l in range(1, shift_time + 1):\n",
        "            dataset[f'sbp[t-{l}]'][i + l] = a\n",
        "    return y_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.options.mode.chained_assignment = None \n",
        "\n",
        "training_list = pd.DataFrame(training_datalist[1:], columns = training_datalist[0])\n",
        "testing_list = pd.DataFrame(testing_datalist[1:], columns = testing_datalist[0])\n",
        "unique_test_subject_id = testing_list['subject_id'].unique().tolist()\n",
        "shift_time = [2, 7, 1, 9, 8, 2, 8, 3, 8, 5, 3]\n",
        "max_degrees = [1, 1, 1, 2, 3, 8, 0, 1, 7, 8, 2]\n",
        "\n",
        "for i in range(len(unique_test_subject_id)):\n",
        "    subject_id = unique_test_subject_id[i]\n",
        "    subject_training_dataset = training_list.loc[training_list['subject_id'] == f'{subject_id}'].reset_index()\n",
        "    subject_testing_dataset = testing_list.loc[testing_list['subject_id'] == f'{subject_id}'].reset_index()\n",
        "    subject_list = pd.concat([subject_training_dataset, subject_testing_dataset])\n",
        "    for j in range(1, shift_time[i] + 1):\n",
        "        subject_list[f'sbp[t-{j}]'] = subject_list['sbp'].shift(j)\n",
        "    subject_list = subject_list.fillna(method='bfill')\n",
        "    subject_list = subject_list[(subject_list['heartrate'] != '')].reset_index()\n",
        "    training_dataset, validation_dataset = SplitData(subject_id, training_list, shift_time[i])\n",
        "    x_train, y_train = PreprocessData(training_dataset, validation_dataset, shift_time[i])\n",
        "    w = MatrixInversion(x_train, y_train, shift_time[i], max_degrees[i])\n",
        "    y_predict = MakePrediction(subject_list, shift_time[i], max_degrees[i], w)\n",
        "    for j in range(len(subject_testing_dataset)):\n",
        "        output_datalist.append([y_predict[len(y_predict) - len(subject_testing_dataset) + j]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
